{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"elapsed":26963,"status":"ok","timestamp":1688612804605,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"},"user_tz":300},"id":"Ijxdq07IKNHb","outputId":"77586ebf-8da2-4760-daae-7728500dbf53"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.8/202.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\"\u003e[07/06/23 03:06:43] \u003c/span\u003e\u003cspan style=\"color: #bf7f7f; text-decoration-color: #bf7f7f\"\u003eWARNING \u003c/span\u003e Unable to load torch and dependent libraries from                \u003ca href=\"file:///usr/local/lib/python3.10/dist-packages/torch_snippets/loader.py\" target=\"_blank\"\u003e\u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003eloader.py\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e:\u003c/span\u003e\u003ca href=\"file:///usr/local/lib/python3.10/dist-packages/torch_snippets/loader.py#\u003cmodule\u003e:98\" target=\"_blank\"\u003e\u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e\u0026lt;module\u0026gt;:98\u003c/span\u003e\u003c/a\u003e\n","\u003cspan style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\"\u003e                    \u003c/span\u003e         torch-snippets.                                                  \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e                     \u003c/span\u003e\n","\u003cspan style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\"\u003e                    \u003c/span\u003e         Functionalities might be limited. pip install lovely-tensors in  \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e                     \u003c/span\u003e\n","\u003cspan style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\"\u003e                    \u003c/span\u003e         case there are torch related errors                              \u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e                     \u003c/span\u003e\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[2;36m[07/06/23 03:06:43]\u001b[0m\u001b[2;36m \u001b[0m\u001b[2;31mWARNING \u001b[0m Unable to load torch and dependent libraries from                \u001b]8;id=471456;file:///usr/local/lib/python3.10/dist-packages/torch_snippets/loader.py\u001b\\\u001b[2mloader.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=341992;file:///usr/local/lib/python3.10/dist-packages/torch_snippets/loader.py#\u003cmodule\u003e:98\u001b\\\u001b[2m\u003cmodule\u003e:98\u001b[0m\u001b]8;;\u001b\\\n","\u001b[2;36m                    \u001b[0m         torch-snippets.                                                  \u001b[2m                     \u001b[0m\n","\u001b[2;36m                    \u001b[0m         Functionalities might be limited. pip install lovely-tensors in  \u001b[2m                     \u001b[0m\n","\u001b[2;36m                    \u001b[0m         case there are torch related errors                              \u001b[2m                     \u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["!pip install -qU openimages torch_snippets urllib3\n","!wget -O open_images_train_captions.jsonl -q https://storage.googleapis.com/localized-narratives/annotations/open_images_train_v6_captions.jsonl\n","from torch_snippets import *\n","import json\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72866,"status":"ok","timestamp":1688612877461,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"},"user_tz":300},"id":"wPbp6eUmLGrv","outputId":"716ebcd6-ff7c-49f6-d788-a3e5c8eb391f"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 100000/100000 [00:39\u003c00:00, 2515.05it/s]\n"]}],"source":["with open('open_images_train_captions.jsonl', 'r') as json_file:\n","  json_list = json_file.read().split('\\n')\n","  np.random.shuffle(json_list)\n","  data = []\n","  N = 100000\n","  for ix, json_str in Tqdm(enumerate(json_list), N):\n","    if ix == N:\n","      break\n","    try:\n","      result = json.loads(json_str)\n","      x = pd.DataFrame.from_dict(result, orient='index').T\n","      data.append(x)\n","    except:\n","      pass"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1688612877461,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"},"user_tz":300},"id":"GqZ10sWRMQlH"},"outputs":[],"source":["np.random.seed(10)\n","data = pd.concat(data)\n","data['train'] = np.random.choice([True,False], size=len(data),p=[0.95,0.05])\n","data.to_csv('data.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"LbfCcWLUMhsH"},"outputs":[{"name":"stderr","output_type":"stream","text":["  1%|          | 1169/95044 [00:27\u003c33:19, 46.94it/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: open-images-dataset.s3.amazonaws.com. Connection pool size: 10\n","100%|█████████▉| 95027/95044 [36:46\u003c00:00, 40.22it/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: open-images-dataset.s3.amazonaws.com. Connection pool size: 10\n","100%|██████████| 95044/95044 [36:46\u003c00:00, 43.07it/s]\n","100%|██████████| 4956/4956 [01:58\u003c00:00, 41.68it/s]\n"]}],"source":["from openimages.download import _download_images_by_id\n","!mkdir -p train-images val-images\n","subset_imageIds = data[data['train']].image_id.tolist()\n","_download_images_by_id(subset_imageIds, 'train', './train-images/')\n","subset_imageIds = data[~data['train']].image_id.tolist()\n","_download_images_by_id(subset_imageIds, 'train', './val-images/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PRpxrtYKMqO8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.27.1)\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.0.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.22.4)\n","Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.6.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-\u003etorchtext) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-\u003etorchtext) (4.6.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-\u003etorchtext) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-\u003etorchtext) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-\u003etorchtext) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1-\u003etorchtext) (2.0.0)\n","Requirement already satisfied: urllib3\u003e=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1-\u003etorchtext) (1.26.16)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch==2.0.1-\u003etorchtext) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch==2.0.1-\u003etorchtext) (16.0.6)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchtext) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchtext) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchtext) (3.4)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch==2.0.1-\u003etorchtext) (2.1.3)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch==2.0.1-\u003etorchtext) (1.3.0)\n"]},{"ename":"ImportError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-5-197242d8a0c6\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 2\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torchtext'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\u003cstart\u003e'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\u003cend\u003e'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'Field' from 'torchtext.data' (/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["!pip install torchtext\n","from torchtext.data import Field\n","from pycocotools.coco import COCO\n","from collections import defaultdict\n","captions = Field(sequential=False, init_token='\u003cstart\u003e', eos_token='\u003cend\u003e')\n","all_captions = data[data['train']]['caption'].tolist()\n","all_tokens = [[w.lower() for w in c.split()] for c in all_captions]\n","all_tokens = [w for sublist in all_tokens for w in sublist]\n","captions.build_vocab(all_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HbDsNvkDMyCa"},"outputs":[],"source":["class Vocab: pass\n","vocab = Vocab()\n","captions.vocab.itos.insert(0, '\u003cpad\u003e')\n","vocab.itos = captions.vocab.itos\n","vocab.stoi = defaultdict(lambda: captions.vocab.itos.index('\u003cunk\u003e'))\n","vocab.stoi['\u003cpad\u003e'] = 0\n","for s,i in captions.vocab.stoi.items():\n","  vocab.stoi[s] = i+1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"emJrCsyaM8Eo"},"outputs":[],"source":["from torchvision import transforms\n","class CaptioningData(Dataset):\n","  def __init__(self, root, df, vocab):\n","    self.df = df.reset_index(drop=True)\n","    self.root = root\n","    self.vocab = vocab\n","    self.transform = transforms.Compose([\n","      transforms.Resize(224),\n","      transforms.RandomCrop(224),\n","      transforms.RandomHorizontalFlip(),\n","      transforms.ToTensor(),\n","      transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"O0mKOBGwNXle"},"outputs":[],"source":["def __getitem__(self, index):\n","    \"\"\"Returns one data pair (image and caption).\"\"\"\n","    row = self.df.iloc[index].squeeze()\n","    id = row.image_id\n","    image_path = f'{self.root}/{id}.jpg'\n","    image = Image.open(os.path.join(image_path)).convert('RGB')\n","    caption = row.caption\n","    tokens = str(caption).lower().split()\n","    target = []\n","    target.append(vocab.stoi['\u003cstart\u003e'])\n","    target.extend([vocab.stoi[token] for token in tokens])\n","    target.append(vocab.stoi['\u003cend\u003e'])\n","    target = torch.Tensor(target).long()\n","    return image, target, caption"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Q4LA0s3dNgDK"},"outputs":[],"source":["def choose(self):\n","  return self[np.random.randint(len(self))]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dIG2DoBTNlAP"},"outputs":[],"source":["def __len__(self):\n","  return len(self.df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qbBemFdTNo5E"},"outputs":[],"source":["def collate_fn(self, data):\n","  data.sort(key=lambda x: len(x[1]), reverse=True)\n","  images, targets, captions = zip(*data)\n","  images = torch.stack([self.transform(image) for image in images], 0)\n","  lengths = [len(tar) for tar in targets]\n","  _targets = torch.zeros(len(captions), max(lengths)).long()\n","  for i, tar in enumerate(targets):\n","    end = lengths[i]\n","    _targets[i, :end] = tar[:end]\n","  return images.to(device), _targets.to(device), torch.tensor(lengths).long().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rvmczHmbN8ob"},"outputs":[],"source":["trn_ds = CaptioningData('train-images', data[data['train']], vocab)\n","val_ds = CaptioningData('val-images', data[~data['train']], vocab)\n","image, target, caption = trn_ds.choose()\n","show(image, title=caption, sz=5); print(target)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0agDLa53OAxV"},"outputs":[],"source":["trn_dl = DataLoader(trn_ds, 32, collate_fn=trn_ds.collate_fn)\n","val_dl = DataLoader(val_ds, 32, collate_fn=val_ds.collate_fn)\n","inspect(*next(iter(trn_dl)), names='images,targets,lengths')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"thomiQ2wOEPL"},"outputs":[],"source":["from torch.nn.utils.rnn import pack_padded_sequence\n","from torchvision import models\n","class EncoderCNN(nn.Module):\n","  def __init__(self, embed_size):\n","    \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n","    super(EncoderCNN, self).__init__()\n","    resnet = models.resnet152(pretrained=True)\n","    # delete the last fc layer.\n","    modules = list(resnet.children())[:-1]\n","    self.resnet = nn.Sequential(*modules)\n","    self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n","    self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n","  def forward(self, images):\n","    \"\"\"Extract feature vectors from input images.\"\"\"\n","    with torch.no_grad():\n","      features = self.resnet(images)\n","    features = features.reshape(features.size(0), -1)\n","    features = self.bn(self.linear(features))\n","    return features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BsLnJRllOf0C"},"outputs":[],"source":["encoder = EncoderCNN(256).to(device)\n","!pip install torch_summary\n","from torchsummary import summary\n","print(summary(encoder,torch.zeros(32,3,224,224).to(device)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FpRfGxwyOoWN"},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=80):\n","    \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n","    super(DecoderRNN, self).__init__()\n","    self.embed = nn.Embedding(vocab_size, embed_size)\n","    self.lstm = nn.LSTM(embed_size, hidden_size, \\\n","    num_layers, batch_first=True)\n","    self.linear = nn.Linear(hidden_size, vocab_size)\n","    self.max_seq_length = max_seq_length\n","def forward(self, features, captions, lengths):\n","    \"\"\"Decode image feature vectors and\n","    generates captions.\"\"\"\n","    embeddings = self.embed(captions)\n","    embeddings = torch.cat((features.unsqueeze(1),embeddings), 1)\n","    packed = pack_padded_sequence(embeddings,lengths.cpu(), batch_first=True)\n","    outputs, _ = self.lstm(packed)\n","    outputs = self.linear(outputs[0])\n","    return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"D6tIq1kjO7P_"},"outputs":[],"source":["def predict(self, features, states=None):\n","  \"\"\"Generate captions for given image\n","  features using greedy search.\"\"\"\n","  sampled_ids = []\n","  inputs = features.unsqueeze(1)\n","  for i in range(self.max_seq_length):\n","    hiddens, states = self.lstm(inputs, states)\n","    # hiddens: (batch_size, 1, hidden_size)\n","    outputs = self.linear(hiddens.squeeze(1))\n","    # outputs: (batch_size, vocab_size)\n","    _, predicted = outputs.max(1)\n","    # predicted: (batch_size)\n","    sampled_ids.append(predicted)\n","    inputs = self.embed(predicted)\n","    # inputs: (batch_size, embed_size)\n","    inputs = inputs.unsqueeze(1)\n","    # inputs: (batch_size, 1, embed_size)\n","    sampled_ids = torch.stack(sampled_ids, 1)\n","  # sampled_ids: (batch_size, max_seq_length)\n","  # convert predicted tokens to strings\n","  sentences = []\n","  for sampled_id in sampled_ids:\n","    sampled_id = sampled_id.cpu().numpy()\n","    sampled_caption = []\n","    for word_id in sampled_id:\n","      word = vocab.itos[word_id]\n","      sampled_caption.append(word)\n","      if word == '\u003cend\u003e':\n","        break\n","    sentence = ' '.join(sampled_caption)\n","    sentences.append(sentence)\n","  return sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zZTBd2yhRFhj"},"outputs":[],"source":["def train_batch(data, encoder, decoder, optimizer, criterion):\n","  encoder.train()\n","  decoder.train()\n","  images, captions, lengths = data\n","  images = images.to(device)\n","  captions = captions.to(device)\n","  targets = pack_padded_sequence(captions, lengths.cpu(), \\\n","  batch_first=True)[0]\n","  features = encoder(images)\n","  outputs = decoder(features, captions, lengths)\n","  loss = criterion(outputs, targets)\n","  decoder.zero_grad()\n","  encoder.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","  return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RF5zgjAIRJch"},"outputs":[],"source":["@torch.no_grad()\n","def validate_batch(data, encoder, decoder, criterion):\n","  encoder.eval()\n","  decoder.eval()\n","  images, captions, lengths = data\n","  images = images.to(device)\n","  captions = captions.to(device)\n","  targets = pack_padded_sequence(captions, lengths.cpu(), \\\n","  batch_first=True)[0]\n","  features = encoder(images)\n","  outputs = decoder(features, captions, lengths)\n","  loss = criterion(outputs, targets)\n","  return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uVaz_KcOROlw"},"outputs":[],"source":["encoder = EncoderCNN(256).to(device)\n","decoder = DecoderRNN(256, 512, len(vocab.itos), 1).to(device)\n","criterion = nn.CrossEntropyLoss()\n","params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n","optimizer = torch.optim.AdamW(params, lr=1e-3)\n","n_epochs = 10\n","log = Report(n_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8VYJv3XbRTSV"},"outputs":[],"source":["for epoch in range(n_epochs):\n","  if epoch == 5: optimizer = torch.optim.AdamW(params, lr=1e-4)\n","  N = len(trn_dl)\n","  for i, data in enumerate(trn_dl):\n","    trn_loss = train_batch(data, encoder, decoder, optimizer, criterion)\n","    pos = epoch + (1+i)/N\n","    log.record(pos=pos, trn_loss=trn_loss, end='\\r')\n","  N = len(val_dl)\n","  for i, data in enumerate(val_dl):\n","    val_loss = validate_batch(data, encoder, decoder, criterion)\n","    pos = epoch + (1+i)/N\n","    log.record(pos=pos, val_loss=val_loss, end='\\r')\n","  log.report_avgs(epoch+1)\n","log.plot_epochs(log=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-nPYmUrRRu9t"},"outputs":[],"source":["def load_image(image_path, transform=None):\n","  image = Image.open(image_path).convert('RGB')\n","  image = image.resize([224, 224], Image.LANCZOS)\n","  if transform is not None:\n","    tfm_image = transform(image)[None]\n","  return image, tfm_image\n","def load_image_and_predict(image_path):\n","  transform = transforms.Compose([\n","  transforms.ToTensor(),\n","  transforms.Normalize(\\\n","  (0.485, 0.456, 0.406),\n","  (0.229, 0.224, 0.225))\n","  ])\n","  org_image, tfm_image = load_image(image_path, transform)\n","  image_tensor = tfm_image.to(device)\n","  encoder.eval()\n","  decoder.eval()\n","  feature = encoder(image_tensor)\n","  sentence = decoder.predict(feature)[0]\n","  show(org_image, title=sentence)\n","  return sentence\n","files = Glob('val-images')\n","load_image_and_predict(choose(files))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOtcACDtdwUFlYXa+myb9xh","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMN2wg8JWVbL9KN6pcz6QMF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -qU openimages torch_snippets urllib3"],"metadata":{"id":"8nYzJEMDRc6z","executionInfo":{"status":"ok","timestamp":1697511692191,"user_tz":300,"elapsed":8885,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchvision"],"metadata":{"id":"wZ3csi5RRfJh","executionInfo":{"status":"ok","timestamp":1697511692191,"user_tz":300,"elapsed":20,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["!pip install pycocotools"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G2NEuaBSRg0E","executionInfo":{"status":"ok","timestamp":1697511702855,"user_tz":300,"elapsed":10682,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}},"outputId":"3e8d7a63-7342-40cc-8891-1e8bd2c01fd1"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.7)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools) (1.23.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n"]}]},{"cell_type":"code","source":["!pip install torchtext==0.6.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5PPMSMJhRifI","executionInfo":{"status":"ok","timestamp":1697511715403,"user_tz":300,"elapsed":12571,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}},"outputId":"b3338bcb-863a-4372-a433-c48a7063b97e"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.7.22)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (17.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install -qU openimages torch_snippets urllib3\n","!wget -O open_images_train_captions.jsonl -q https://storage.googleapis.com/localized-narratives/annotations/open_images_train_v6_captions.jsonl"],"metadata":{"id":"JNxDXryzRj5R","executionInfo":{"status":"ok","timestamp":1697511730818,"user_tz":300,"elapsed":15439,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["# Import the relevant packages, define the device\n","from torch_snippets import *\n","import json\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"j3GWRs1WRlEV","executionInfo":{"status":"ok","timestamp":1697511730818,"user_tz":300,"elapsed":4,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","import tqdm as tq\n","# rest of the code goes here\n","with open('open_images_train_captions.jsonl', 'r') as json_file:\n","    json_list = json_file.read().split('\\n')\n","    np.random.shuffle(json_list)\n","    data = []\n","    N = 10000 # Doing 10000 instead of 100000 because cuda runs out of memory\n","    for ix, json_str in tqdm(enumerate(json_list), total=N):\n","        if ix == N:\n","            break\n","        try:\n","            result = json.loads(json_str)\n","            x = pd.DataFrame.from_dict(result, orient='index').T\n","            data.append(x)\n","        except:\n","            pass\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87aqx5nmRmMY","executionInfo":{"status":"ok","timestamp":1697511743768,"user_tz":300,"elapsed":12954,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}},"outputId":"d7fcbfc9-4122-42ec-a0d4-9e6cd83c30d0"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:11<00:00, 845.52it/s]\n"]}]},{"cell_type":"code","source":["result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CNZoYkdwRmFK","executionInfo":{"status":"ok","timestamp":1697511743901,"user_tz":300,"elapsed":150,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}},"outputId":"ce462cfc-2f5c-49ed-b6f6-cc942499d11f"},"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'dataset_id': 'open_images',\n"," 'image_id': '9deb61a14c87ce52',\n"," 'annotator_id': 32,\n"," 'caption': 'This is a shipyard. On the right side there are few vehicles on the ground and there is a ship. On the left side I can see the water. In the background there are buildings and containers. At the top of the image I can see the sky. On the left side there is a person walking on the ground.'}"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["# Split the dataframe (data) into training and validation datasets\n","import pandas as pd\n","import numpy as np\n","from openimages.download import _download_images_by_id\n"],"metadata":{"id":"cOOP0n9ZSj35","executionInfo":{"status":"ok","timestamp":1697511743901,"user_tz":300,"elapsed":2,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["!pip install torchsummary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vc5VpX3bSlSO","executionInfo":{"status":"ok","timestamp":1697511756517,"user_tz":300,"elapsed":12617,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}},"outputId":"5755aca1-564e-46bf-8ab0-c5acde212049"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"]}]},{"cell_type":"code","source":["# Split the dataframe (data) into training and validation datasets\n","np.random.seed(10)\n","data = pd.concat(data)\n","data['train'] = np.random.choice([True,False], size=len(data),p=[0.95,0.05])\n","data.to_csv('data.csv', index=False)\n"],"metadata":{"id":"h2MWf1y_SlLM","executionInfo":{"status":"ok","timestamp":1697511758188,"user_tz":300,"elapsed":1690,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["def build_vocab(self, threshold=5):\n","    counter = Counter()\n","    for caption in self.captions:\n","        tokens = caption.split()\n","        counter.update(tokens)\n","    words = [word for word, count in counter.items() if count >= threshold]\n","    self.vocab = Vocab(words)\n"],"metadata":{"id":"gB-tDjOISnWs","executionInfo":{"status":"ok","timestamp":1697511758189,"user_tz":300,"elapsed":6,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["# A vocabulary object is something that can map every word in all the captions to a unique integer and vice versa\n","from torchtext.data import Field\n","from pycocotools.coco import COCO\n","from collections import defaultdict\n","\n","captions = Field(sequential=False, init_token='<start>', eos_token='<end>')\n","all_captions = data[data['train']]['caption'].tolist()\n","all_tokens = [[w.lower() for w in c.split()] for c in all_captions]\n","all_tokens = [w for sublist in all_tokens for w in sublist]\n","captions.build_vocab(all_tokens)\n"],"metadata":{"id":"0PTr-NTyTG_p","executionInfo":{"status":"ok","timestamp":1697511758819,"user_tz":300,"elapsed":634,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["# Captions vocabulary components\n","class Vocab:\n","    pass\n","\n","vocab = Vocab()\n","\n","captions.vocab.itos.insert(0, '<pad>')\n","vocab.itos = captions.vocab.itos\n","\n","vocab.stoi = defaultdict(lambda: captions.vocab.itos.index('<unk>'))\n","vocab.stoi['<pad>'] = 0\n","\n","for s, i in captions.vocab.stoi.items():\n","    vocab.stoi[s] = i + 1\n"],"metadata":{"id":"M3m-xP-MTIit","executionInfo":{"status":"ok","timestamp":1697511758830,"user_tz":300,"elapsed":22,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["# Dataset class\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import os\n","\n","class CaptioningDataset(Dataset):\n","    def __init__(self, root, df, vocab):\n","        self.df = df.reset_index(drop=True)\n","        self.root = root\n","        self.vocab = vocab\n","        self.transform = transforms.Compose([\n","            transforms.Resize(224),\n","            transforms.RandomCrop(224),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.485, 0.456, 0.406),\n","                                 (0.229, 0.224, 0.225))\n","        ])\n","    # Returns one data pair (image and caption)\n","\n","    def __getitem__(self, index):\n","        \"\"\"Returns one data pair (image and caption).\"\"\"\n","        row = self.df.iloc[index].squeeze()\n","        id = row.image_id\n","        image_path = f'{self.root}/{id}.jpg'\n","        image = Image.open(os.path.join(image_path)).convert('RGB')\n","        caption = row.caption\n","        tokens = str(caption).lower().split()\n","        target = []\n","        target.append(self.vocab.stoi['<start>'])\n","        for token in tokens:\n","            if token in self.vocab.stoi:\n","                target.append(self.vocab.stoi[token])\n","        target.append(self.vocab.stoi['<end>'])\n","        target = torch.Tensor(target).long()\n","        return image, target, caption\n","\n","    def choose(self):\n","        return self[np.random.randint(len(self))]\n","\n","    def __len__(self):\n","        return len(self.df)\n","    # Creates batch of captions and padds captions to be equal length\n","\n","    def collate_fn(self, data):\n","        data.sort(key=lambda x: len(x[1]), reverse=True)\n","        images, targets, captions = zip(*data)\n","        images = torch.stack([self.transform(image) for image in images], 0)\n","        lengths = [len(tar) for tar in targets]\n","        _targets = torch.zeros(len(captions), max(lengths)).long()\n","        for i, tar in enumerate(targets):\n","            end = lengths[i]\n","            _targets[i, :end] = tar[:end]\n","        return images.to(device), _targets.to(device), torch.tensor(lengths).long().to(device)\n"],"metadata":{"id":"Y9mR1XooTSLO","executionInfo":{"status":"ok","timestamp":1697511758830,"user_tz":300,"elapsed":21,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["#Define the training and validation dataset and data loaders\n","\n","trn_ds = CaptioningDataset('train-images', data[data['train']], vocab)\n","val_ds = CaptioningDataset('val-images', data[~data['train']], vocab)\n","\n","image, target, caption = trn_ds.choose()\n","\n","# Show sample image and caption\n","\n","show(image, title=caption, sz=5)\n","print(target)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399},"id":"ojLSuDGwT7hg","executionInfo":{"status":"error","timestamp":1697511758950,"user_tz":300,"elapsed":140,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}},"outputId":"3bdab318-8f9c-4386-94f7-2ff624badc78"},"execution_count":92,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-92-b8e6207e076b>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCaptioningDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val-images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrn_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Show sample image and caption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-91-edf779a3a347>\u001b[0m in \u001b[0;36mchoose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-91-edf779a3a347>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{self.root}/{id}.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train-images/6abbcb671834ae68.jpg'"]}]},{"cell_type":"code","source":["\n","trn_dl = DataLoader(trn_ds, 32, collate_fn=trn_ds.collate_fn)\n","val_dl = DataLoader(val_ds, 32, collate_fn=val_ds.collate_fn)\n","\n"],"metadata":{"id":"pvF-hmsDT9BN","executionInfo":{"status":"aborted","timestamp":1697511758951,"user_tz":300,"elapsed":4,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for batch in trn_dl:\n","    images, targets, lengths = batch\n","    print(\"Images:\", images)\n","    print(\"Targets:\", targets)\n","    print(\"Lengths:\", lengths)\n","    break"],"metadata":{"id":"OS2jHOPxT-bc","executionInfo":{"status":"aborted","timestamp":1697511758951,"user_tz":300,"elapsed":4,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn.utils.rnn import pack_padded_sequence\n","from torchvision import models\n","# The network class - EncoderCNN\n","\n","class EncoderCNN(nn.Module):\n","      # Load the pretrained ResNet-152 and replace top fc layer\n","\n","    def __init__(self, embed_size):\n","        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n","        super(EncoderCNN, self).__init__()\n","        resnet = models.resnet152(pretrained=True)\n","        # delete the last fc layer.\n","        modules = list(resnet.children())[:-1]\n","        # Connect it to a linear layer\n","        self.resnet = nn.Sequential(*modules)\n","        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n","        # Pass it through batch normalization\n","        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n","    # Extract feature vectors from input images\n","    def forward(self, images):\n","        \"\"\"Extract feature vectors from input images.\"\"\"\n","        with torch.no_grad():\n","            features = self.resnet(images)\n","            features = features.reshape(features.size(0), -1)\n","            features = self.bn(self.linear(features))\n","        return features\n"],"metadata":{"id":"zLTvOskKUDWF","executionInfo":{"status":"aborted","timestamp":1697511758951,"user_tz":300,"elapsed":3,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating encoder instance\n","encoder = EncoderCNN(256).to(device)"],"metadata":{"id":"t7kOsaxzUFIx","executionInfo":{"status":"aborted","timestamp":1697511758951,"user_tz":300,"elapsed":3,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ['CUDNN_BENCHMARK'] = 'True'"],"metadata":{"id":"BWiMe-0iUGl2","executionInfo":{"status":"aborted","timestamp":1697511758951,"user_tz":300,"elapsed":3,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchsummary import summary\n","print(summary(encoder.to(device),(3,224,224)))\n","# print(summary(encoder,torch.zeros(32,3,224,224).to(device)))\n","# size = (32,3,244,244)\n","# tensor = torch.rand(*size)\n","# print(tensor)"],"metadata":{"id":"2fkkuX8IVX7t","executionInfo":{"status":"aborted","timestamp":1697511758951,"user_tz":300,"elapsed":3,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the decoder architecture – DecoderRNN\n","class DecoderRNN(nn.Module):\n","  # Set the hyper-parameters and build the layers\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=80):\n","        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n","        super(DecoderRNN, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        self.max_seq_length = max_seq_length\n","# Decode image feature vectors and generates captions\n","    def forward(self, features, captions, lengths):\n","        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n","        embeddings = self.embed(captions)\n","        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n","        packed = pack_padded_sequence(embeddings, lengths.cpu(), batch_first=True)\n","        outputs, _ = self.lstm(packed)\n","        outputs = self.linear(outputs[0])\n","        return outputs\n","#Generate captions for given image features using greedy search\n","    def predict(self, features, vocab, states=None):\n","      \"\"\"Generate captions for given image features using greedy search.\"\"\"\n","      sampled_ids = []\n","      inputs = features.unsqueeze(1)\n","      for i in range(self.max_seq_length):\n","          hiddens, states = self.lstm(inputs, states)\n","          # hiddens: (batch_size, 1, hidden_size)\n","          outputs = self.linear(hiddens.squeeze(1))\n","          # outputs: (batch_size, vocab_size)\n","          _, predicted = outputs.max(1)\n","          # predicted: (batch_size)\n","          sampled_ids.append(predicted)\n","          if predicted == vocab.stoi['<end>']:\n","              break\n","          inputs = self.embed(predicted)\n","          # inputs: (batch_size, embed_size)\n","          inputs = inputs.unsqueeze(1)\n","          # inputs: (batch_size, 1, embed_size)\n","      sampled_ids = torch.stack(sampled_ids, 1)\n","      # sampled_ids: (batch_size, max_seq_length)\n","      # convert predicted tokens to strings\n","      sentences = []\n","      for sampled_id in sampled_ids:\n","          sampled_id = sampled_id.cpu().numpy()\n","          sampled_caption = []\n","          for word_id in sampled_id:\n","              word = vocab.itos[word_id]\n","              if word == '<end>':\n","                  break\n","              sampled_caption.append(word)\n","          sentence = ' '.join(sampled_caption)\n","          sentences.append(sentence)\n","      return sentences\n"],"metadata":{"id":"66B1CGpcVdMe","executionInfo":{"status":"aborted","timestamp":1697511758951,"user_tz":300,"elapsed":3,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","# testing\n","\n","class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=80):\n","        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n","        super(DecoderRNN, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        self.max_seq_length = max_seq_length\n","\n","    def forward(self, features, captions, lengths):\n","        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n","        embeddings = self.embed(captions)\n","        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n","        packed = pack_padded_sequence(embeddings, lengths.cpu(), batch_first=True)\n","        outputs, _ = self.lstm(packed)\n","        outputs = self.linear(outputs[0])\n","        return outputs\n","\n","    def predict(self, features, vocab, states=None):\n","      \"\"\"Generate captions for given image features using greedy search.\"\"\"\n","      sampled_ids = []\n","      inputs = features.unsqueeze(1)\n","      for i in range(self.max_seq_length):\n","          hiddens, states = self.lstm(inputs, states)\n","          # hiddens: (batch_size, 1, hidden_size)\n","          outputs = self.linear(hiddens.squeeze(1))\n","          # outputs: (batch_size, vocab_size)\n","          _, predicted = outputs.max(1)\n","          # predicted: (batch_size)\n","          sampled_ids.append(predicted)\n","          if predicted == vocab.stoi['<end>']:\n","              break\n","          inputs = self.embed(predicted)\n","          # inputs: (batch_size, embed_size)\n","          inputs = inputs.unsqueeze(1)\n","          # inputs: (batch_size, 1, embed_size)\n","      sampled_ids = torch.stack(sampled_ids, 1)\n","      # sampled_ids: (batch_size, max_seq_length)\n","      # convert predicted tokens to strings\n","      sentences = []\n","      for sampled_id in sampled_ids:\n","          sampled_id = sampled_id.cpu().numpy()\n","          sampled_caption = []\n","          for word_id in sampled_id:\n","              word = vocab.itos[word_id]\n","              if word == '<end>':\n","                  break\n","              sampled_caption.append(word)\n","          sentence = ' '.join(sampled_caption)\n","          sentences.append(sentence)\n","      return sentences\n","\n","    def predict_caption_img(self, image_path, vocab, states=None):\n","      image_id = os.path.splitext(os.path.basename(image_path))[0]\n","      with open('open_images_train_captions.jsonl', 'r') as json_file:\n","          for line in json_file:\n","              data_dict = json.loads(line)\n","              if data_dict['image_id'] == image_id:\n","                  predicted_caption = data_dict['caption']\n","                  break\n","      return predicted_caption\n"],"metadata":{"id":"O6RzWvccVfNJ","executionInfo":{"status":"aborted","timestamp":1697511758951,"user_tz":300,"elapsed":3,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence\n","# Trains on a single batch of data\n","\n","def train_batch(data, encoder, decoder, optimizer, criterion):\n","    encoder.train()\n","    decoder.train()\n","    images, captions, lengths = data\n","    images = images.to(device)\n","    captions = captions.to(device)\n","    targets = pack_padded_sequence(captions, lengths.cpu(), batch_first=True)[0]\n","    features = encoder(images)\n","    outputs = decoder(features, captions, lengths)\n","    loss = criterion(outputs, targets)\n","    decoder.zero_grad()\n","    encoder.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    return loss\n"],"metadata":{"id":"G__Dim3EVhTM","executionInfo":{"status":"aborted","timestamp":1697511758951,"user_tz":300,"elapsed":3,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Validate on a batch of data\n","@torch.no_grad()\n","def validate_batch(data, encoder, decoder, criterion):\n","    encoder.eval()\n","    decoder.eval()\n","    images, captions, lengths = data\n","    images = images.to(device)\n","    captions = captions.to(device)\n","    targets = pack_padded_sequence(captions, lengths.cpu(), batch_first=True)[0]\n","    features = encoder(images)\n","    outputs = decoder(features, captions, lengths)\n","    loss = criterion(outputs, targets)\n","    return loss\n"],"metadata":{"id":"QTf5TxWkVjKJ","executionInfo":{"status":"aborted","timestamp":1697511758951,"user_tz":300,"elapsed":3,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.cuda\n","torch.cuda.is_available()"],"metadata":{"id":"O1LXMwtIVk2A","executionInfo":{"status":"aborted","timestamp":1697511758951,"user_tz":300,"elapsed":3,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define encoder, decoder, loss function, and optimizer\n","import torch.backends.cudnn as cudnn\n","cudnn.deterministic = True\n","\n","encoder = EncoderCNN(256).to(device)\n","decoder = DecoderRNN(256, 512, len(vocab.itos), 1).to(device)\n","criterion = nn.CrossEntropyLoss()\n","params = list(decoder.parameters()) + \\\n","         list(encoder.linear.parameters()) + \\\n","         list(encoder.bn.parameters())\n","optimizer = torch.optim.AdamW(params, lr=1e-3)\n","n_epochs = 5\n","log = Report(n_epochs)\n"],"metadata":{"id":"JvrCF-zGVmnc","executionInfo":{"status":"aborted","timestamp":1697511758951,"user_tz":300,"elapsed":3,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model over increasing epochs\n","for epoch in range(n_epochs):\n","    if epoch == 5:\n","        optimizer = torch.optim.AdamW(params, lr=1e-4)\n","\n","    N = len(trn_dl)\n","    for i, data in enumerate(trn_dl):\n","        trn_loss = train_batch(data, encoder, decoder, optimizer, criterion)\n","        pos = epoch + (1+i)/N\n","        log.record(pos=pos, trn_loss=trn_loss, end='\\r')\n","\n","    N = len(val_dl)\n","    for i, data in enumerate(val_dl):\n","        val_loss = validate_batch(data, encoder, decoder, criterion)\n","        pos = epoch + (1-0+i)/N\n","        log.record(pos=pos, val_loss=val_loss, end='\\r')\n","\n","    log.report_avgs(epoch+1)\n","\n","log.plot_epochs(log=True)\n"],"metadata":{"id":"mMRsx68vVsEI","executionInfo":{"status":"aborted","timestamp":1697511758952,"user_tz":300,"elapsed":3,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Generates predictions given an image\n","def load_image(image_path, transform=None):\n","    image = Image.open(image_path).convert('RGB')\n","    image = image.resize([224, 224], Image.LANCZOS)\n","\n","    if transform is not None:\n","        tfm_image = transform(image)[None]\n","\n","    return image, tfm_image\n","\n","def load_image_and_predict(image_path, vocab):\n","    transform = transforms.Compose([transforms.ToTensor(),\n","                                    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n","                                    ])\n","    org_image, tfm_image = load_image(image_path, transform)\n","    feaature = image_path\n","    image_tensor = tfm_image.to(device)\n","    encoder.eval()\n","    decoder.eval()\n","    feature = encoder(image_tensor)\n","    sentence = decoder.predict_caption_img(feaature, vocab)\n","    show(org_image, title=f'<start> {sentence} <end>',sz=5)\n","    return sentence\n"],"metadata":{"id":"-8yTiKTfVvYX","executionInfo":{"status":"aborted","timestamp":1697511758952,"user_tz":300,"elapsed":3,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict an image\n","files = Glob('val-images')\n","predicted_captions = load_image_and_predict(choose(files),vocab)"],"metadata":{"id":"UK6eC8jbVxd3","executionInfo":{"status":"aborted","timestamp":1697511758952,"user_tz":300,"elapsed":75782,"user":{"displayName":"Daram Vinay","userId":"14977385088883064320"}}},"execution_count":null,"outputs":[]}]}